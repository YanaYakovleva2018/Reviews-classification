{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras import losses, metrics, optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Load text data and labels from CSV files\n",
    "df_text = pd.read_csv('test_reviews.csv')\n",
    "df_labels = pd.read_csv('test_labels_pred.csv')\n",
    "\n",
    "# Create a mapping for sentiment labels (Negative -> 0, Positive -> 1)\n",
    "label_mapping = {'Negative': 0, 'Positive': 1}\n",
    "df_labels['sentiment'] = df_labels['sentiment'].map(label_mapping)\n",
    "\n",
    "# Merge text data and labels on the 'id' column\n",
    "merged_df = pd.merge(df_text, df_labels, on='id')\n",
    "\n",
    "# Define the fraction of data to be used for training\n",
    "train_fraction = 0.8\n",
    "train_size = int(len(merged_df) * train_fraction)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_data = merged_df['text'][:train_size]\n",
    "train_labels = merged_df['sentiment'][:train_size]\n",
    "test_data = merged_df['text'][train_size:]\n",
    "test_labels = merged_df['sentiment'][train_size:]\n",
    "\n",
    "# Define the maximum number of words to tokenize\n",
    "max_words = 10000  \n",
    "\n",
    "# Initialize a tokenizer and fit it on the training data\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(train_data)\n",
    "\n",
    "# Convert text data to binary matrix representation\n",
    "x_train = tokenizer.texts_to_matrix(train_data, mode='binary')\n",
    "x_test = tokenizer.texts_to_matrix(test_data, mode='binary')\n",
    "\n",
    "# Convert labels to NumPy arrays of float32\n",
    "y_train = np.asarray(train_labels).astype('float32')\n",
    "y_test = np.asarray(test_labels).astype('float32')\n",
    "\n",
    "# Create a Sequential model for binary classification\n",
    "model = Sequential()\n",
    "model.add(Dense(16, activation='relu', input_shape=(max_words,)))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model with RMSprop optimizer, binary cross-entropy loss, and binary accuracy metric\n",
    "model.compile(optimizer=optimizers.RMSprop(lr=0.001),\n",
    "              loss=losses.binary_crossentropy,\n",
    "              metrics=[metrics.binary_accuracy])\n",
    "\n",
    "# Train the model on the training data\n",
    "history = model.fit(x_train,\n",
    "                    y_train,\n",
    "                    epochs=100,\n",
    "                    batch_size=512,\n",
    "                    validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "df_text = pd.read_csv('test_reviews.csv')\n",
    "df_labels = pd.read_csv('test_labels_pred.csv')\n",
    "merged_df = pd.merge(df_text, df_labels, on='id')\n",
    "\n",
    "max_words = 10000 \n",
    "# Initialize a tokenizer and fit it on the 'text' column of the merged dataframe\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(merged_df['text'])\n",
    "\n",
    "# Specify the row number of the text to decode (0-based index)\n",
    "row_number_to_decode = 0 \n",
    "\n",
    "# Retrieve the text from the specified row\n",
    "chosen_text = merged_df['text'][row_number_to_decode]\n",
    "\n",
    "# Tokenize the chosen text into sequences using the trained tokenizer\n",
    "chosen_text_data = tokenizer.texts_to_sequences([chosen_text])\n",
    "\n",
    "# Create a reverse word index to map numeric tokens back to words\n",
    "reverse_word_index = {v: k for k, v in tokenizer.word_index.items()}\n",
    "\n",
    "# Decode the numeric sequence back into text, replacing unknown words with '?'\n",
    "decoded_review = ' '.join([reverse_word_index.get(i, '?') for i in chosen_text_data[0]])\n",
    "\n",
    "# Print the decoded review\n",
    "print(decoded_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test data to calculate test loss and accuracy\n",
    "test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
    "\n",
    "# Print the test loss and test accuracy\n",
    "print(\"Loss :\",test_loss)\n",
    "print(\"Accuracy :\",test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model to a file named 'model.h5'\n",
    "model.save('model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
